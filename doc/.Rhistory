for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2],depth_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.25))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(par=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
View(err_cv)
which.min(err_cv[,1])
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
fit_train <- train(dat_train, label_train, par_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
par_best$par
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file="../output/fit_train.RData")
tm_test=NA
if(run.test){
load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file="../output/pred_test.RData")
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
require("pacman")
install.packages("pacman")
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages(c("acs", "backports", "bayesplot", "BH", "bit", "bit64", "blogdown", "bookdown", "boot", "broom", "cairoDevice", "car", "caTools", "checkmate", "choroplethr", "chron", "cli", "cluster", "coin", "colourpicker", "countrycode", "curl", "data.table", "dendextend", "devtools", "digest", "doParallel", "dplyr", "DT", "dtplyr", "dygraphs", "ellipse", "evaluate", "factoextra", "FactoMineR", "fansi", "fftwtools", "flexmix", "foreach", "foreign", "formatR", "Formula", "fpc", "gbm", "gdata", "gender", "geosphere", "ggplot2", "ggpubr", "ggrepel", "ggsci", "git2r", "gridExtra", "gtools", "highr", "Hmisc", "htmlTable", "htmlwidgets", "httpuv", "hunspell", "igraph", "inline", "irlba", "iterators", "janeaustenr", "kernlab", "knitr", "lattice", "lazyeval", "lme4", "lmtest", "loo", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "matrixStats", "mclust", "memoise", "mgcv", "mime", "miniUI", "modeltools", "multcomp", "munsell", "mvtnorm", "nloptr", "NLP", "NMF", "OAIHarvester", "openNLPdata", "openssl", "packrat", "pbkrtest", "pkgmaker", "PKI", "plotrix", "ps", "psych", "qdap", "qdapDictionaries", "qdapRegex", "qdapTools", "quantreg", "R6", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "registry", "remotes", "reprex", "reshape2", "rgdal", "rgeos", "RgoogleMaps", "rJava", "rjson", "rlang", "rmarkdown", "rngtools", "robustbase", "rpart", "rpart.plot", "rprojroot", "rsconnect", "rscopus", "rstantools", "rstudioapi", "sandwich", "scales", "scatterplot3d", "selectr", "servr", "shiny", "shinyjs", "shinystan", "sm", "sourcetools", "sp", "SparseM", "statmod", "stringdist", "stringi", "stringr", "survival", "syuzhet", "testthat", "TH.data", "threejs", "tidyr", "tidyselect", "tidytext", "tm", "tokenizers", "topicmodels", "trimcluster", "viridis", "WDI", "withr", "wordcloud", "xlsx", "XML", "xml2", "xtable", "xts", "yaml", "zoo"))
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}y
install.packages("pacman")
library(packman)
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
install.packages("devtools")
install.packages("yaml")
install.packages(c("bit", "cairoDevice", "caTools", "chron", "curl", "data.table", "digest", "dplyr", "fansi", "foreign", "gbm", "ggplot2", "ggrepel", "git2r", "gtools", "igraph", "irlba", "kernlab", "lattice", "lme4", "lmtest", "mapproj", "maps", "maptools", "MASS", "Matrix", "matrixStats", "mclust", "mgcv", "mime", "mvtnorm", "nloptr", "NMF", "openssl", "ps", "quantreg", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "rgdal", "rgeos", "rJava", "rjson", "rlang", "robustbase", "rstan", "rstanarm", "scales", "sentimentr", "slam", "sm", "sourcetools", "sp", "StanHeaders", "stringdist", "stringi", "stringr", "survival", "testthat", "tidyr", "tidyselect", "tm", "tokenizers", "wordcloud", "XML", "xml2", "xts", "zoo"))
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
library(devtools)
install.packages("base64enc")
install.packages("devtools")
install.packages("processx")
install.packages("devtools")
install.packages("backports")
install.packages("devtools")
install.packages("glue")
install.packages("devtools")
library(devtools)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #97 files in total
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[3])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
install.packages("pacman")
install.packages("pacman")
library(pacman)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec, ifCleanToken)) # source code of ifCleanToken in in lib folder
tesseract_vec
tesseract_if_clean
sum(tesseract_if_clean == TRUE)
sum(tesseract_if_clean == FALSE)
library(tm)
install.packages("tm")
library(tm)
data("AssociatedPress", package = "topicmodels")
library(topicmodels)
install.packages("topicmodels")
data("AssociatedPress", package = "topicmodels")
AssociatedPress
current_ground_truth_txt
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt", "", file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec, ifCleanToken)) # source code of ifCleanToken in in lib folder
sum(tesseract_if_clean == TRUE)
sum(tesseract_if_clean == FALSE)
source('../lib/ifCleanToken.R')
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt", "", file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec, ifCleanToken)) # source code of ifCleanToken in in lib folder
sum(tesseract_if_clean == TRUE)
sum(tesseract_if_clean == FALSE)
tesseract_if_clean == FALSE
tesseract_if_clean$FALSE
tesseract_if_clean
tesseract_if_clean
get_deletion <- function(error_word) {
return(deltable[deltable$Key == error_word, ])
}
library(tm)
library(topicmodels)
library(stringr)
data('AssociatedPress', package = 'topicmodels')
terms <- Terms(AssociatedPress) #extract the unqiue terms
length(terms)
dictionary <- terms
library(dplyr)
library(tidytext)
ap_td <- tidy(AssociatedPress)
ap_td  #the matrix: the document, the term, the count
tf <- ap_td %>%
count(term, sort =F, wt=count)
head(tf, 20) # the frequency of the term
a <- 'apple'
nchar(a)
###############################################
############deletion table#####################
###############################################
###calculate how many rows in the deletion table
allterm <- as.data.frame(tf)
wordlength <- function(wordlist){
sum <- 0
for (i in 1:nrow(wordlist)){
sum <- sum + nchar(wordlist$term[i])
}
return(sum)
}
nallchar <- wordlength(allterm)
######for one word find the deletion cadidate######
candidate <- function(word){
cand <- data.frame(matrix(ncol = 3, nrow = nchar(word)))
for (i in 1:nchar(word)){
word1 <- word
str_sub(word1, i, i) <- ''
cand[i,] <- c(word1, substr(word, i,i), i-1)
}
return(cand)
}
candidate(a)
##########get the table for all terms#######
deltable <- data.frame(matrix(ncol = 3, nrow = nallchar))
len <- rep(NA, nrow(allterm))
for (t in 1:nrow(allterm)){
if (t == 1){
len[t] <- nchar(allterm$term[t])
deltable[1:len[t], ] <- candidate(allterm$term[t])
}else{
len[t] <- nchar(allterm$term[t])
deltable[(sum(len[1:(t-1)])+1):sum(len[1:t]), ] <- candidate(allterm$term[t])
}
}
######check the table##############
colnames(deltable) <- c('Key', 'Correction', 'Position')
head(deltable, 50)
###### get deletion candidate ######
get_deletion <- function(error_word) {
return(deltable[deltable$Key == error_word, ])
}
allterm <- as.data.frame(tf)
library(tm)
library(topicmodels)
library(stringr)
data('AssociatedPress', package = 'topicmodels')
terms <- Terms(AssociatedPress) #extract the unqiue terms
length(terms)
dictionary <- terms
library(dplyr)
install.packages("dplyr")
library(dplyr)
library(tidytext)
install.packages("tidytext")
library(tidytext)
ap_td <- tidy(AssociatedPress)
ap_td  #the matrix: the document, the term, the count
tf <- ap_td %>%
count(term, sort =F, wt=count)
tf <- ap_td %>%
count(term, sort =F, wt=count)
head(tf, 20) # the frequency of the term
a <- 'apple'
nchar(a)
allterm <- as.data.frame(tf)
wordlength <- function(wordlist){
sum <- 0
for (i in 1:nrow(wordlist)){
sum <- sum + nchar(wordlist$term[i])
}
return(sum)
}
nallchar <- wordlength(allterm)
candidate <- function(word){
cand <- data.frame(matrix(ncol = 3, nrow = nchar(word)))
for (i in 1:nchar(word)){
word1 <- word
str_sub(word1, i, i) <- ''
cand[i,] <- c(word1, substr(word, i,i), i-1)
}
return(cand)
}
candidate(a)
deltable <- data.frame(matrix(ncol = 3, nrow = nallchar))
len <- rep(NA, nrow(allterm))
for (t in 1:nrow(allterm)){
if (t == 1){
len[t] <- nchar(allterm$term[t])
deltable[1:len[t], ] <- candidate(allterm$term[t])
}else{
len[t] <- nchar(allterm$term[t])
deltable[(sum(len[1:(t-1)])+1):sum(len[1:t]), ] <- candidate(allterm$term[t])
}
}
colnames(deltable) <- c('Key', 'Correction', 'Position')
head(deltable, 50)
get_deletion <- function(error_word) {
return(deltable[deltable$Key == error_word, ])
}
View(deltable)
library(stringi)
test <- vector("aron", "a", 1)
test <- vector("aron", "a", "1")
test <- vector(, 3)
test
test[1] <- "aron"
test
test[2] <- "a"
test[3] <- "1"
test
candidate <- function(row){
can <- vector(, length = 2)
can[1] <- row[1]
can[2] <- paste(stri_sub(row[1], 1, row[3] - 1), row[2],
stri_sub(row[1], row[3], length(row[1])))
return(can)
}
candidate(test)
candidate <- function(row){
can <- vector(, length = 2)
can[1] <- row[1]
can[2] <- paste(stri_sub(row[1], 1, as.numeric(row[3]) - 1), row[2],
stri_sub(row[1], row[3], length(row[1])))
return(can)
}
candidate(test)
if(as.numeric(row[3] == 0)){
can[2] <- paste(row[2], row[1])
}
if(as.numeric(row[3] == 1)){
can[2] <- paste(stri_sub(row[1], 1, 1), row[2],
stri_sub(row[1], 2, length(row[1])))
}
else{
can[2] <- paste(stri_sub(row[1], 1, as.numeric(row[3])), row[2],
stri_sub(row[1], as.numeric(row[3]), length(row[1])))
}
return(can)
}
candidate <- function(row){
can <- vector(, length = 2)
can[1] <- row[1]
if(as.numeric(row[3] == 0)){
can[2] <- paste(row[2], row[1])
}
if(as.numeric(row[3] == 1)){
can[2] <- paste(stri_sub(row[1], 1, 1), row[2],
stri_sub(row[1], 2, length(row[1])))
}
else{
can[2] <- paste(stri_sub(row[1], 1, as.numeric(row[3])), row[2],
stri_sub(row[1], as.numeric(row[3]), length(row[1])))
}
return(can)
}
test
candidate(test)
stri_sub(test[1], 1, 1)
stri_sub(test[1], 2, length(test[1]))
test[2]
test
stri_sub(test[1], 2, length(test[1]))
length(test[1])
length(as.string(test[1]))
length(as.character(test[1]))
test[1]
paste(stri_sub(test[1], 1, 1), test[2])
stri_sub(test[1], 1, 1)
test[2]
paste0(stri_sub(test[1], 1, 1), test[2])
typeof(test[1])
nchar(test[1])
candidate <- function(row){
can <- vector(, length = 2)
can[1] <- row[1]
if(as.numeric(row[3] == 0)){
can[2] <- paste0(row[2], row[1])
}
if(as.numeric(row[3] == 1)){
can[2] <- paste0(stri_sub(row[1], 1, 1), row[2],
stri_sub(row[1], 2, nchar(row[1])))
}
else{
can[2] <- paste0(stri_sub(row[1], 1, as.numeric(row[3])), row[2],
stri_sub(row[1], as.numeric(row[3]), nchar(row[1])))
}
return(can)
}
test
candidate(test)
test <- vector(3)
test <- vector(,3)
test <- ("abandn", "o", "5")
test <- c("abandn", "o", "5")
candidate(test)
stri_sub(test[1], 1, as.numeric(test[3]))
test[2]
stri_sub(test[1], as.numeric(test[3]), nchar(test[1]))
candidate <- function(row){
can <- vector(, length = 2)
can[1] <- row[1]
if(as.numeric(row[3] == 0)){
can[2] <- paste0(row[2], row[1])
}
if(as.numeric(row[3] == 1)){
can[2] <- paste0(stri_sub(row[1], 1, 1), row[2],
stri_sub(row[1], 2, nchar(row[1])))
}
else{
can[2] <- paste0(stri_sub(row[1], 1, as.numeric(row[3])), row[2],
stri_sub(row[1], as.numeric(row[3]) + 1, nchar(row[1])))
}
return(can)
}
candidate(test)
test <- vector(,3)
test <- c("aron", "a", "0")
candidate(test)
